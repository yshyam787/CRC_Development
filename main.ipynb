{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRC Development #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import yaml #handling yaml files. \n",
    "import pandas as pd\n",
    "from psycopg2 import sql\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from directory_config import DATA_CONFIGURATION_DIRECTORY, DATABASE_CONFIGURATION_DIRECTORY #import root directory and configuraiton.json file. \n",
    "\n",
    "#Mapping SQL data types from YAML to Pandas/Numpy types. \n",
    "SQL_TO_PANDAS_DTYPES  = {\n",
    "    \"FLOAT\": \"float64\",\n",
    "    \"INTEGER\": \"Int64\", #Nullable integer type \n",
    "    \"VARCHAR(50)\": \"string\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Defining functions.   ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Function to load the datasets configuration file (e.g. irish_data.yaml) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_file_path): #file path of irish_data.yaml file\n",
    "    \"\"\"Load YAML configuration file.\"\"\"\n",
    "    with open(config_file_path, 'r') as file: #opening the file in the read mode.\n",
    "        print(f\"Following Configuration files has been loaded successfully: \\n{file}\")\n",
    "        return yaml.safe_load(file) #loading the cofiguration_yaml file. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Function to create the database from database configuration file (e.g. irish_db.yaml) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection to the PostgreSQL database is made. Then, the a cursor is iterated throughout the configuration file ```irish_db.yaml``` and schema is created in PostgreSQL, column definitions are extracted from configuration file, and tables are created accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_database(database_config):\n",
    "    \"\"\"Create database tables using SQLAlchemy to align with df.to_sql in write_to_postgres.\"\"\"\n",
    "    try:\n",
    "        # Create database connection using SQLAlchemy\n",
    "        engine = create_engine(\n",
    "            f\"postgresql://{database_config['database']['user']}:{database_config['database']['password']}@\"\n",
    "            f\"{database_config['database']['host']}:{database_config['database']['port']}/{database_config['database']['dbname']}\"\n",
    "        )\n",
    "        \n",
    "        print(\"Database connected successfully!\")\n",
    "\n",
    "        # Iterate over tables to create them dynamically\n",
    "        for table_name, table_data in database_config[\"tables\"].items():\n",
    "            columns = table_data[\"columns\"]\n",
    "            \n",
    "            # Convert schema dictionary to DataFrame for df.to_sql()\n",
    "            df_schema = pd.DataFrame(columns=columns.keys())\n",
    "            df_schema.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "            \n",
    "            print(f\"Table {table_name} has been created successfully.\")\n",
    "\n",
    "        print(\"All tables have been successfully created using df.to_sql.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating database tables: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Function to import the dataset into the dataframe and preprocess. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_type(df, column_types):\n",
    "    \"\"\"Convert DataFrame columns to expected data types based on YAML config.\"\"\"\n",
    "    for col, dtype in column_types.items():\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                # Convert to FLOAT (coerce invalid values to NaN)\n",
    "                if dtype == \"float64\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "                # Convert to Nullable INTEGER (avoids NaN conversion error)\n",
    "                elif dtype == \"Int64\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")  # Nullable integer type\n",
    "\n",
    "                # Convert to STRING\n",
    "                elif dtype.startswith(\"string\"):\n",
    "                    df[col] = df[col].astype(\"string\").fillna(\"\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Warning: Could not convert column {col} to {dtype}. Error: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(data_config):\n",
    "    \"\"\"Read and load all datasets specified in the YAML file into Pandas DataFrames.\"\"\"\n",
    "    try:\n",
    "        datasets = {} #Declare a dictionary for storing datasets names and respective dataframes. \n",
    "        \n",
    "        for dataset_name, dataset_info in data_config[\"datasets\"].items():\n",
    "            file_name = dataset_info[\"file_name\"]\n",
    "            file_path = dataset_info[\"path\"]\n",
    "            column_name = dataset_info[\"columns\"].items()\n",
    "            file_type = dataset_info[\"file_type\"]\n",
    "            delimiter = dataset_info[\"delimiter\"]\n",
    "            \n",
    "\n",
    "            #Read CSV and Excel files\n",
    "            if file_type == \"csv\":\n",
    "                df = pd.read_csv(file_path, delimiter = delimiter, dtype=str) #Read all as string (by default).\n",
    "            elif file_type == \"excel\":\n",
    "                df = pd.read_excel(file_path, engine=\"openpyxl\", dtype=str)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file type: {file_type}\")\n",
    "            \n",
    "            #Transformations (if any). It seems that file_specific script can also be associated to the file here for preprocessing and related transformations.\n",
    "            \n",
    "            #Example: Convert all column names to lowercase and replace '.' with '_' in names. \n",
    "            df.columns = [col.lower() for col in df.columns]\n",
    "            df.columns = [col.replace('.', '_') for col in df.columns]\n",
    "            df[\"file_origin\"] = file_name #Addding the filenames in the new column.\n",
    "            print(f\"Following is the preprocessed column name:{df.columns}\\n\")\n",
    "            \n",
    "            # Extract expected data types from YAML\n",
    "            column_types = {\n",
    "                col: SQL_TO_PANDAS_DTYPES.get(dtype.split()[0], \"string\")\n",
    "                for col, dtype in column_name\n",
    "            }\n",
    "\n",
    "            print(f\"üîπ Applying data types for {dataset_name}: {column_types}\\n\")\n",
    "\n",
    "            # Convert DataFrame columns to match expected types\n",
    "            df = convert_column_type(df, column_types)\n",
    "\n",
    "\n",
    "            #Store the dataset_name and respective dataframe in dictionary. \n",
    "            datasets[dataset_name] = df\n",
    "            print(f\"Loaded dataset: {dataset_name} | Shape: {df.shape}\\n\")\n",
    "            print(f\"Datasets data: {datasets}\\n\")\n",
    "\n",
    "        return datasets\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Function to write the DataFrames to PostgreSQL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_postgres(db_config, datasets_dict):\n",
    "    \"\"\"Write DataFrame to PostgreSQL using bulk ingestion for efficiency.\"\"\"\n",
    "    try:\n",
    "        # Create database connection using SQLAlchemy for bulk operations\n",
    "        engine = create_engine(\n",
    "            f\"postgresql://{db_config['database']['user']}:{db_config['database']['password']}@\"\n",
    "            f\"{db_config['database']['host']}:{db_config['database']['port']}/{db_config['database']['dbname']}\"\n",
    "        )\n",
    "        \n",
    "        print(\"Connected to the database successfully!\")\n",
    "\n",
    "        # Iterate over tables\n",
    "        for table_name in db_config['tables'].keys():\n",
    "            if table_name in datasets_dict:\n",
    "                df = datasets_dict[table_name]\n",
    "                \n",
    "                # Use df.to_sql for bulk insertion\n",
    "                df.to_sql(table_name, engine, if_exists='append', index=False, method='multi')\n",
    "                \n",
    "                print(f\"Bulk inserted data into {table_name} successfully.\")\n",
    "\n",
    "        print(\"Data written to PostgreSQL successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to PostgreSQL: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Defining ```main``` function ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the ```load_config``` function are called to load the datasets from the datasets_configuration file. ```create_database``` function is called to create the databse tables, and ```import_dataset``` function is called to process and import the datasets. Finally, the data is written to PostgreSQL using ```write_to_postgres``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Load the configuration file\n",
    "    db_config = load_config(DATABASE_CONFIGURATION_DIRECTORY)\n",
    "    data_config = load_config(DATA_CONFIGURATION_DIRECTORY)\n",
    "    print(db_config)\n",
    "    print(data_config)\n",
    "\n",
    "    #Load the database configuration (irish_db.yaml) and create database tables.\n",
    "\n",
    "    load_dotenv() #load the .env file for the credentials. \n",
    "    db_config[\"database\"][\"user\"] = os.getenv(\"DB_USER\")\n",
    "    db_config[\"database\"][\"password\"] = os.getenv(\"DB_PASSWORD\")\n",
    "    \n",
    "    create_database(db_config)\n",
    "    print(create_database)\n",
    "\n",
    "    #Imports datasets and preprocess it. \n",
    "    datasets_dict = import_dataset(data_config)\n",
    "\n",
    "    #Write data to PostgreSQL\n",
    "    write_to_postgres(db_config, datasets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following Configuration files has been loaded successfully: \n",
      "<_io.TextIOWrapper name='./config/irish_db.yaml' mode='r' encoding='UTF-8'>\n",
      "Following Configuration files has been loaded successfully: \n",
      "<_io.TextIOWrapper name='./config/irish_data.yaml' mode='r' encoding='UTF-8'>\n",
      "{'database': {'host': 'localhost', 'port': 5444, 'dbname': 'irish_db', 'user': '${DB_USER}', 'password': '${DB_PASSWORD}'}, 'tables': {'irish': {'columns': {'file_origin': 'VARCHAR(50)', 'sepal_length': 'FLOAT', 'sepal_width': 'FLOAT', 'petal_length': 'FLOAT', 'petal_width': 'FLOAT', 'variety': 'VARCHAR(50)'}}, 'mtcars': {'columns': {'file_origin': 'VARCHAR(50)', 'model': 'VARCHAR(50)', 'mpg': 'FLOAT', 'cyl': 'INTEGER', 'disp': 'FLOAT', 'hp': 'INTEGER', 'drat': 'FLOAT', 'wt': 'FLOAT', 'qsec': 'FLOAT', 'vs': 'INTEGER', 'am': 'INTEGER', 'gear': 'INTEGER', 'carb': 'INTEGER'}}, 'weather': {'columns': {'file_origin': 'VARCHAR(50)', 'mintemp': 'FLOAT', 'maxtemp': 'FLOAT', 'rainfall': 'FLOAT', 'evaporation': 'FLOAT', 'sunshine': 'FLOAT', 'windgustdir': 'VARCHAR(50)', 'windgustspeed': 'INTEGER', 'winddir9am': 'VARCHAR(50)', 'winddir3pm': 'VARCHAR(50)', 'windspeed9am': 'INTEGER', 'windspeed3pm': 'INTEGER', 'humidity9am': 'INTEGER', 'humidity3pm': 'INTEGER', 'pressure9am': 'FLOAT', 'pressure3pm': 'FLOAT', 'cloud9am': 'INTEGER', 'cloud3pm': 'INTEGER', 'temp9am': 'FLOAT', 'temp3pm': 'FLOAT', 'raintoday': 'VARCHAR(50)', 'risk_mm': 'FLOAT', 'raintomorrow': 'VARCHAR(50)'}}}}\n",
      "{'datasets': {'irish': {'file_name': 'irish.csv', 'path': 'irish_data/irish.csv', 'file_type': 'csv', 'delimiter': ',', 'columns': {'sepal_length': 'FLOAT', 'sepal_width': 'FLOAT', 'petal_length': 'FLOAT', 'petal_width': 'FLOAT', 'variety': 'VARCHAR(50)'}}, 'mtcars': {'file_name': 'mtcars.csv', 'path': 'irish_data/mtcars.csv', 'file_type': 'csv', 'delimiter': ',', 'columns': {'model': 'VARCHAR(50)', 'mpg': 'FLOAT', 'cyl': 'INTEGER', 'disp': 'FLOAT', 'hp': 'INTEGER', 'drat': 'FLOAT', 'wt': 'FLOAT', 'qsec': 'FLOAT', 'vs': 'INTEGER', 'am': 'INTEGER', 'gear': 'INTEGER', 'carb': 'INTEGER'}}, 'weather': {'file_name': 'weather.csv', 'path': 'irish_data/irish_weather/weather.csv', 'file_type': 'csv', 'delimiter': ',', 'columns': {'mintemp': 'FLOAT', 'maxtemp': 'FLOAT', 'rainfall': 'FLOAT', 'evaporation': 'FLOAT', 'sunshine': 'FLOAT', 'windgustdir': 'VARCHAR(50)', 'windgustspeed': 'INTEGER', 'winddir9am': 'VARCHAR(50)', 'winddir3pm': 'VARCHAR(50)', 'windspeed9am': 'INTEGER', 'windspeed3pm': 'INTEGER', 'humidity9am': 'INTEGER', 'humidity3pm': 'INTEGER', 'pressure9am': 'FLOAT', 'pressure3pm': 'FLOAT', 'cloud9am': 'INTEGER', 'cloud3pm': 'INTEGER', 'temp9am': 'FLOAT', 'temp3pm': 'FLOAT', 'raintoday': 'VARCHAR(50)', 'risk_mm': 'FLOAT', 'raintomorrow': 'VARCHAR(50)'}}}}\n",
      "Database connected successfully!\n",
      "Table irish has been created successfully.\n",
      "Table mtcars has been created successfully.\n",
      "Table weather has been created successfully.\n",
      "All tables have been successfully created using df.to_sql.\n",
      "<function create_database at 0x14c3136a0>\n",
      "Following is the preprocessed column name:Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'variety',\n",
      "       'file_origin'],\n",
      "      dtype='object')\n",
      "\n",
      "üîπ Applying data types for irish: {'sepal_length': 'float64', 'sepal_width': 'float64', 'petal_length': 'float64', 'petal_width': 'float64', 'variety': 'string'}\n",
      "\n",
      "Loaded dataset: irish | Shape: (150, 6)\n",
      "\n",
      "Datasets data: {'irish':      sepal_length  sepal_width  petal_length  petal_width    variety  \\\n",
      "0             5.1          3.5           1.4          0.2     Setosa   \n",
      "1             4.9          3.0           1.4          0.2     Setosa   \n",
      "2             4.7          3.2           1.3          0.2     Setosa   \n",
      "3             4.6          3.1           1.5          0.2     Setosa   \n",
      "4             5.0          3.6           1.4          0.2     Setosa   \n",
      "..            ...          ...           ...          ...        ...   \n",
      "145           6.7          3.0           5.2          2.3  Virginica   \n",
      "146           6.3          2.5           5.0          1.9  Virginica   \n",
      "147           6.5          3.0           5.2          2.0  Virginica   \n",
      "148           6.2          3.4           5.4          2.3  Virginica   \n",
      "149           5.9          3.0           5.1          1.8  Virginica   \n",
      "\n",
      "    file_origin  \n",
      "0     irish.csv  \n",
      "1     irish.csv  \n",
      "2     irish.csv  \n",
      "3     irish.csv  \n",
      "4     irish.csv  \n",
      "..          ...  \n",
      "145   irish.csv  \n",
      "146   irish.csv  \n",
      "147   irish.csv  \n",
      "148   irish.csv  \n",
      "149   irish.csv  \n",
      "\n",
      "[150 rows x 6 columns]}\n",
      "\n",
      "Following is the preprocessed column name:Index(['model', 'mpg', 'cyl', 'disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am',\n",
      "       'gear', 'carb', 'file_origin'],\n",
      "      dtype='object')\n",
      "\n",
      "üîπ Applying data types for mtcars: {'model': 'string', 'mpg': 'float64', 'cyl': 'Int64', 'disp': 'float64', 'hp': 'Int64', 'drat': 'float64', 'wt': 'float64', 'qsec': 'float64', 'vs': 'Int64', 'am': 'Int64', 'gear': 'Int64', 'carb': 'Int64'}\n",
      "\n",
      "Loaded dataset: mtcars | Shape: (32, 13)\n",
      "\n",
      "Datasets data: {'irish':      sepal_length  sepal_width  petal_length  petal_width    variety  \\\n",
      "0             5.1          3.5           1.4          0.2     Setosa   \n",
      "1             4.9          3.0           1.4          0.2     Setosa   \n",
      "2             4.7          3.2           1.3          0.2     Setosa   \n",
      "3             4.6          3.1           1.5          0.2     Setosa   \n",
      "4             5.0          3.6           1.4          0.2     Setosa   \n",
      "..            ...          ...           ...          ...        ...   \n",
      "145           6.7          3.0           5.2          2.3  Virginica   \n",
      "146           6.3          2.5           5.0          1.9  Virginica   \n",
      "147           6.5          3.0           5.2          2.0  Virginica   \n",
      "148           6.2          3.4           5.4          2.3  Virginica   \n",
      "149           5.9          3.0           5.1          1.8  Virginica   \n",
      "\n",
      "    file_origin  \n",
      "0     irish.csv  \n",
      "1     irish.csv  \n",
      "2     irish.csv  \n",
      "3     irish.csv  \n",
      "4     irish.csv  \n",
      "..          ...  \n",
      "145   irish.csv  \n",
      "146   irish.csv  \n",
      "147   irish.csv  \n",
      "148   irish.csv  \n",
      "149   irish.csv  \n",
      "\n",
      "[150 rows x 6 columns], 'mtcars':                   model   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  \\\n",
      "0             Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1   \n",
      "1         Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1   \n",
      "2            Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1   \n",
      "3        Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0   \n",
      "4     Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0   \n",
      "5               Valiant  18.1    6  225.0  105  2.76  3.460  20.22   1   0   \n",
      "6            Duster 360  14.3    8  360.0  245  3.21  3.570  15.84   0   0   \n",
      "7             Merc 240D  24.4    4  146.7   62  3.69  3.190  20.00   1   0   \n",
      "8              Merc 230  22.8    4  140.8   95  3.92  3.150  22.90   1   0   \n",
      "9              Merc 280  19.2    6  167.6  123  3.92  3.440  18.30   1   0   \n",
      "10            Merc 280C  17.8    6  167.6  123  3.92  3.440  18.90   1   0   \n",
      "11           Merc 450SE  16.4    8  275.8  180  3.07  4.070  17.40   0   0   \n",
      "12           Merc 450SL  17.3    8  275.8  180  3.07  3.730  17.60   0   0   \n",
      "13          Merc 450SLC  15.2    8  275.8  180  3.07  3.780  18.00   0   0   \n",
      "14   Cadillac Fleetwood  10.4    8  472.0  205  2.93  5.250  17.98   0   0   \n",
      "15  Lincoln Continental  10.4    8  460.0  215  3.00  5.424  17.82   0   0   \n",
      "16    Chrysler Imperial  14.7    8  440.0  230  3.23  5.345  17.42   0   0   \n",
      "17             Fiat 128  32.4    4   78.7   66  4.08  2.200  19.47   1   1   \n",
      "18          Honda Civic  30.4    4   75.7   52  4.93  1.615  18.52   1   1   \n",
      "19       Toyota Corolla  33.9    4   71.1   65  4.22  1.835  19.90   1   1   \n",
      "20        Toyota Corona  21.5    4  120.1   97  3.70  2.465  20.01   1   0   \n",
      "21     Dodge Challenger  15.5    8  318.0  150  2.76  3.520  16.87   0   0   \n",
      "22          AMC Javelin  15.2    8  304.0  150  3.15  3.435  17.30   0   0   \n",
      "23           Camaro Z28  13.3    8  350.0  245  3.73  3.840  15.41   0   0   \n",
      "24     Pontiac Firebird  19.2    8  400.0  175  3.08  3.845  17.05   0   0   \n",
      "25            Fiat X1-9  27.3    4   79.0   66  4.08  1.935  18.90   1   1   \n",
      "26        Porsche 914-2  26.0    4  120.3   91  4.43  2.140  16.70   0   1   \n",
      "27         Lotus Europa  30.4    4   95.1  113  3.77  1.513  16.90   1   1   \n",
      "28       Ford Pantera L  15.8    8  351.0  264  4.22  3.170  14.50   0   1   \n",
      "29         Ferrari Dino  19.7    6  145.0  175  3.62  2.770  15.50   0   1   \n",
      "30        Maserati Bora  15.0    8  301.0  335  3.54  3.570  14.60   0   1   \n",
      "31           Volvo 142E  21.4    4  121.0  109  4.11  2.780  18.60   1   1   \n",
      "\n",
      "    gear  carb file_origin  \n",
      "0      4     4  mtcars.csv  \n",
      "1      4     4  mtcars.csv  \n",
      "2      4     1  mtcars.csv  \n",
      "3      3     1  mtcars.csv  \n",
      "4      3     2  mtcars.csv  \n",
      "5      3     1  mtcars.csv  \n",
      "6      3     4  mtcars.csv  \n",
      "7      4     2  mtcars.csv  \n",
      "8      4     2  mtcars.csv  \n",
      "9      4     4  mtcars.csv  \n",
      "10     4     4  mtcars.csv  \n",
      "11     3     3  mtcars.csv  \n",
      "12     3     3  mtcars.csv  \n",
      "13     3     3  mtcars.csv  \n",
      "14     3     4  mtcars.csv  \n",
      "15     3     4  mtcars.csv  \n",
      "16     3     4  mtcars.csv  \n",
      "17     4     1  mtcars.csv  \n",
      "18     4     2  mtcars.csv  \n",
      "19     4     1  mtcars.csv  \n",
      "20     3     1  mtcars.csv  \n",
      "21     3     2  mtcars.csv  \n",
      "22     3     2  mtcars.csv  \n",
      "23     3     4  mtcars.csv  \n",
      "24     3     2  mtcars.csv  \n",
      "25     4     1  mtcars.csv  \n",
      "26     5     2  mtcars.csv  \n",
      "27     5     2  mtcars.csv  \n",
      "28     5     4  mtcars.csv  \n",
      "29     5     6  mtcars.csv  \n",
      "30     5     8  mtcars.csv  \n",
      "31     4     2  mtcars.csv  }\n",
      "\n",
      "Following is the preprocessed column name:Index(['mintemp', 'maxtemp', 'rainfall', 'evaporation', 'sunshine',\n",
      "       'windgustdir', 'windgustspeed', 'winddir9am', 'winddir3pm',\n",
      "       'windspeed9am', 'windspeed3pm', 'humidity9am', 'humidity3pm',\n",
      "       'pressure9am', 'pressure3pm', 'cloud9am', 'cloud3pm', 'temp9am',\n",
      "       'temp3pm', 'raintoday', 'risk_mm', 'raintomorrow', 'file_origin'],\n",
      "      dtype='object')\n",
      "\n",
      "üîπ Applying data types for weather: {'mintemp': 'float64', 'maxtemp': 'float64', 'rainfall': 'float64', 'evaporation': 'float64', 'sunshine': 'float64', 'windgustdir': 'string', 'windgustspeed': 'Int64', 'winddir9am': 'string', 'winddir3pm': 'string', 'windspeed9am': 'Int64', 'windspeed3pm': 'Int64', 'humidity9am': 'Int64', 'humidity3pm': 'Int64', 'pressure9am': 'float64', 'pressure3pm': 'float64', 'cloud9am': 'Int64', 'cloud3pm': 'Int64', 'temp9am': 'float64', 'temp3pm': 'float64', 'raintoday': 'string', 'risk_mm': 'float64', 'raintomorrow': 'string'}\n",
      "\n",
      "Loaded dataset: weather | Shape: (366, 23)\n",
      "\n",
      "Datasets data: {'irish':      sepal_length  sepal_width  petal_length  petal_width    variety  \\\n",
      "0             5.1          3.5           1.4          0.2     Setosa   \n",
      "1             4.9          3.0           1.4          0.2     Setosa   \n",
      "2             4.7          3.2           1.3          0.2     Setosa   \n",
      "3             4.6          3.1           1.5          0.2     Setosa   \n",
      "4             5.0          3.6           1.4          0.2     Setosa   \n",
      "..            ...          ...           ...          ...        ...   \n",
      "145           6.7          3.0           5.2          2.3  Virginica   \n",
      "146           6.3          2.5           5.0          1.9  Virginica   \n",
      "147           6.5          3.0           5.2          2.0  Virginica   \n",
      "148           6.2          3.4           5.4          2.3  Virginica   \n",
      "149           5.9          3.0           5.1          1.8  Virginica   \n",
      "\n",
      "    file_origin  \n",
      "0     irish.csv  \n",
      "1     irish.csv  \n",
      "2     irish.csv  \n",
      "3     irish.csv  \n",
      "4     irish.csv  \n",
      "..          ...  \n",
      "145   irish.csv  \n",
      "146   irish.csv  \n",
      "147   irish.csv  \n",
      "148   irish.csv  \n",
      "149   irish.csv  \n",
      "\n",
      "[150 rows x 6 columns], 'mtcars':                   model   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  \\\n",
      "0             Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1   \n",
      "1         Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1   \n",
      "2            Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1   \n",
      "3        Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0   \n",
      "4     Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0   \n",
      "5               Valiant  18.1    6  225.0  105  2.76  3.460  20.22   1   0   \n",
      "6            Duster 360  14.3    8  360.0  245  3.21  3.570  15.84   0   0   \n",
      "7             Merc 240D  24.4    4  146.7   62  3.69  3.190  20.00   1   0   \n",
      "8              Merc 230  22.8    4  140.8   95  3.92  3.150  22.90   1   0   \n",
      "9              Merc 280  19.2    6  167.6  123  3.92  3.440  18.30   1   0   \n",
      "10            Merc 280C  17.8    6  167.6  123  3.92  3.440  18.90   1   0   \n",
      "11           Merc 450SE  16.4    8  275.8  180  3.07  4.070  17.40   0   0   \n",
      "12           Merc 450SL  17.3    8  275.8  180  3.07  3.730  17.60   0   0   \n",
      "13          Merc 450SLC  15.2    8  275.8  180  3.07  3.780  18.00   0   0   \n",
      "14   Cadillac Fleetwood  10.4    8  472.0  205  2.93  5.250  17.98   0   0   \n",
      "15  Lincoln Continental  10.4    8  460.0  215  3.00  5.424  17.82   0   0   \n",
      "16    Chrysler Imperial  14.7    8  440.0  230  3.23  5.345  17.42   0   0   \n",
      "17             Fiat 128  32.4    4   78.7   66  4.08  2.200  19.47   1   1   \n",
      "18          Honda Civic  30.4    4   75.7   52  4.93  1.615  18.52   1   1   \n",
      "19       Toyota Corolla  33.9    4   71.1   65  4.22  1.835  19.90   1   1   \n",
      "20        Toyota Corona  21.5    4  120.1   97  3.70  2.465  20.01   1   0   \n",
      "21     Dodge Challenger  15.5    8  318.0  150  2.76  3.520  16.87   0   0   \n",
      "22          AMC Javelin  15.2    8  304.0  150  3.15  3.435  17.30   0   0   \n",
      "23           Camaro Z28  13.3    8  350.0  245  3.73  3.840  15.41   0   0   \n",
      "24     Pontiac Firebird  19.2    8  400.0  175  3.08  3.845  17.05   0   0   \n",
      "25            Fiat X1-9  27.3    4   79.0   66  4.08  1.935  18.90   1   1   \n",
      "26        Porsche 914-2  26.0    4  120.3   91  4.43  2.140  16.70   0   1   \n",
      "27         Lotus Europa  30.4    4   95.1  113  3.77  1.513  16.90   1   1   \n",
      "28       Ford Pantera L  15.8    8  351.0  264  4.22  3.170  14.50   0   1   \n",
      "29         Ferrari Dino  19.7    6  145.0  175  3.62  2.770  15.50   0   1   \n",
      "30        Maserati Bora  15.0    8  301.0  335  3.54  3.570  14.60   0   1   \n",
      "31           Volvo 142E  21.4    4  121.0  109  4.11  2.780  18.60   1   1   \n",
      "\n",
      "    gear  carb file_origin  \n",
      "0      4     4  mtcars.csv  \n",
      "1      4     4  mtcars.csv  \n",
      "2      4     1  mtcars.csv  \n",
      "3      3     1  mtcars.csv  \n",
      "4      3     2  mtcars.csv  \n",
      "5      3     1  mtcars.csv  \n",
      "6      3     4  mtcars.csv  \n",
      "7      4     2  mtcars.csv  \n",
      "8      4     2  mtcars.csv  \n",
      "9      4     4  mtcars.csv  \n",
      "10     4     4  mtcars.csv  \n",
      "11     3     3  mtcars.csv  \n",
      "12     3     3  mtcars.csv  \n",
      "13     3     3  mtcars.csv  \n",
      "14     3     4  mtcars.csv  \n",
      "15     3     4  mtcars.csv  \n",
      "16     3     4  mtcars.csv  \n",
      "17     4     1  mtcars.csv  \n",
      "18     4     2  mtcars.csv  \n",
      "19     4     1  mtcars.csv  \n",
      "20     3     1  mtcars.csv  \n",
      "21     3     2  mtcars.csv  \n",
      "22     3     2  mtcars.csv  \n",
      "23     3     4  mtcars.csv  \n",
      "24     3     2  mtcars.csv  \n",
      "25     4     1  mtcars.csv  \n",
      "26     5     2  mtcars.csv  \n",
      "27     5     2  mtcars.csv  \n",
      "28     5     4  mtcars.csv  \n",
      "29     5     6  mtcars.csv  \n",
      "30     5     8  mtcars.csv  \n",
      "31     4     2  mtcars.csv  , 'weather':      mintemp  maxtemp  rainfall  evaporation  sunshine windgustdir  \\\n",
      "0        8.0     24.3       0.0          3.4       6.3          NW   \n",
      "1       14.0     26.9       3.6          4.4       9.7         ENE   \n",
      "2       13.7     23.4       3.6          5.8       3.3          NW   \n",
      "3       13.3     15.5      39.8          7.2       9.1          NW   \n",
      "4        7.6     16.1       2.8          5.6      10.6         SSE   \n",
      "..       ...      ...       ...          ...       ...         ...   \n",
      "361      9.0     30.7       0.0          7.6      12.1         NNW   \n",
      "362      7.1     28.4       0.0         11.6      12.7           N   \n",
      "363     12.5     19.9       0.0          8.4       5.3         ESE   \n",
      "364     12.5     26.9       0.0          5.0       7.1          NW   \n",
      "365     12.3     30.2       0.0          6.0      12.6          NW   \n",
      "\n",
      "     windgustspeed winddir9am winddir3pm  windspeed9am  ...  pressure9am  \\\n",
      "0               30         SW         NW             6  ...       1019.7   \n",
      "1               39          E          W             4  ...       1012.4   \n",
      "2               85          N        NNE             6  ...       1009.5   \n",
      "3               54        WNW          W            30  ...       1005.5   \n",
      "4               50        SSE        ESE            20  ...       1018.3   \n",
      "..             ...        ...        ...           ...  ...          ...   \n",
      "361             76        SSE         NW             7  ...       1016.1   \n",
      "362             48        NNW        NNW             2  ...       1020.0   \n",
      "363             43        ENE        ENE            11  ...       1024.0   \n",
      "364             46        SSW        WNW             6  ...       1021.0   \n",
      "365             78         NW        WNW            31  ...       1009.6   \n",
      "\n",
      "     pressure3pm  cloud9am  cloud3pm  temp9am  temp3pm  raintoday  risk_mm  \\\n",
      "0         1015.0         7         7     14.4     23.6         No      3.6   \n",
      "1         1008.4         5         3     17.5     25.7        Yes      3.6   \n",
      "2         1007.2         8         7     15.4     20.2        Yes     39.8   \n",
      "3         1007.0         2         7     13.5     14.1        Yes      2.8   \n",
      "4         1018.5         7         7     11.1     15.4        Yes      0.0   \n",
      "..           ...       ...       ...      ...      ...        ...      ...   \n",
      "361       1010.8         1         3     20.4     30.0         No      0.0   \n",
      "362       1016.9         0         1     17.2     28.2         No      0.0   \n",
      "363       1022.8         3         2     14.5     18.3         No      0.0   \n",
      "364       1016.2         6         7     15.8     25.9         No      0.0   \n",
      "365       1009.2         1         1     23.8     28.6         No      0.0   \n",
      "\n",
      "     raintomorrow  file_origin  \n",
      "0             Yes  weather.csv  \n",
      "1             Yes  weather.csv  \n",
      "2             Yes  weather.csv  \n",
      "3             Yes  weather.csv  \n",
      "4              No  weather.csv  \n",
      "..            ...          ...  \n",
      "361            No  weather.csv  \n",
      "362            No  weather.csv  \n",
      "363            No  weather.csv  \n",
      "364            No  weather.csv  \n",
      "365            No  weather.csv  \n",
      "\n",
      "[366 rows x 23 columns]}\n",
      "\n",
      "Connected to the database successfully!\n",
      "Bulk inserted data into irish successfully.\n",
      "Bulk inserted data into mtcars successfully.\n",
      "Bulk inserted data into weather successfully.\n",
      "Data written to PostgreSQL successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run and verify the pipeline ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that you are connected to the VM with PostgreSQL database. Also, the tunneling (```ssh```) is done so that the database created by you can be pushed to the database. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
